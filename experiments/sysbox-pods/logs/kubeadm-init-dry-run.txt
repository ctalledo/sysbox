# K8s Kubeadm Config

```
root@k8s-master:~# more kubeadm-conf.yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: "v1.18.2"
networking:
  podSubnet: "10.245.0.0/16"
  serviceSubnet: "10.97.0.0/12"
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
clusterDNS:
- 10.97.0.10
```


```
root@k8s-master:~# kubeadm init --dry-run --config kubeadm-conf.yaml
W0305 21:11:46.802854    5321 utils.go:26] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.96.0.10]; the provided value is: [10.97.0.10]
W0305 21:11:46.803249    5321 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[init] Using Kubernetes version: v1.18.2
[preflight] Running pre-flight checks
[preflight] Would pull the required images (like 'kubeadm config images pull')
[kubelet-start] Writing kubelet environment file with flags to file "/etc/kubernetes/tmp/kubeadm-init-dryrun674923880/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/etc/kubernetes/tmp/kubeadm-init-dryrun674923880/config.yaml"
[certs] Using certificateDir folder "/etc/kubernetes/tmp/kubeadm-init-dryrun674923880"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key

[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.244.0.38]   <<< ERROR: WHY 10.96.0.1 ?

[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [10.244.0.38 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [10.244.0.38 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes/tmp/kubeadm-init-dryrun674923880"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/tmp/kubeadm-init-dryrun674923880"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
W0305 21:11:52.745724    5321 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[control-plane] Creating static Pod manifest for "kube-scheduler"
W0305 21:11:52.749268    5321 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[dryrun] Would ensure that "/var/lib/etcd" directory is present
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/tmp/kubeadm-init-dryrun674923880"
[dryrun] Wrote certificates, kubeconfig files and control plane manifests to the "/etc/kubernetes/tmp/kubeadm-init-dryrun674923880" directory
[dryrun] The certificates or kubeconfig files would not be printed due to their sensitive nature
[dryrun] Please examine the "/etc/kubernetes/tmp/kubeadm-init-dryrun674923880" directory for details about what would be written


[dryrun] Would write file "/etc/kubernetes/manifests/kube-apiserver.yaml" with content:
        apiVersion: v1
        kind: Pod
        metadata:
          annotations:
            kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.244.0.38:6443
          creationTimestamp: null
          labels:
            component: kube-apiserver
            tier: control-plane
          name: kube-apiserver
          namespace: kube-system
        spec:
          containers:
          - command:
            - kube-apiserver
            - --advertise-address=10.244.0.38
            - --allow-privileged=true
            - --authorization-mode=Node,RBAC
            - --client-ca-file=/etc/kubernetes/pki/ca.crt
            - --enable-admission-plugins=NodeRestriction
            - --enable-bootstrap-token-auth=true
            - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
            - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
            - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
            - --etcd-servers=https://127.0.0.1:2379
            - --insecure-port=0
            - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
            - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
            - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
            - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
            - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
            - --requestheader-allowed-names=front-proxy-client
            - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
            - --requestheader-extra-headers-prefix=X-Remote-Extra-
            - --requestheader-group-headers=X-Remote-Group
            - --requestheader-username-headers=X-Remote-User
            - --secure-port=6443
            - --service-account-key-file=/etc/kubernetes/pki/sa.pub
            - --service-cluster-ip-range=10.97.0.0/12
            - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
            - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
            image: k8s.gcr.io/kube-apiserver:v1.18.2
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 8
              httpGet:
                host: 10.244.0.38
                path: /healthz
                port: 6443
                scheme: HTTPS
              initialDelaySeconds: 15
              timeoutSeconds: 15
            name: kube-apiserver
            resources:
              requests:
                cpu: 250m
            volumeMounts:
            - mountPath: /etc/ssl/certs
              name: ca-certs
              readOnly: true
            - mountPath: /etc/ca-certificates
              name: etc-ca-certificates
              readOnly: true
            - mountPath: /etc/kubernetes/pki
              name: k8s-certs
              readOnly: true
            - mountPath: /usr/local/share/ca-certificates
              name: usr-local-share-ca-certificates
              readOnly: true
            - mountPath: /usr/share/ca-certificates
              name: usr-share-ca-certificates
              readOnly: true
          hostNetwork: true
          priorityClassName: system-cluster-critical
          volumes:
          - hostPath:
              path: /etc/ssl/certs
              type: DirectoryOrCreate
            name: ca-certs
          - hostPath:
              path: /etc/ca-certificates
              type: DirectoryOrCreate
            name: etc-ca-certificates
          - hostPath:
              path: /etc/kubernetes/pki
              type: DirectoryOrCreate
            name: k8s-certs
          - hostPath:
              path: /usr/local/share/ca-certificates
              type: DirectoryOrCreate
            name: usr-local-share-ca-certificates
          - hostPath:
              path: /usr/share/ca-certificates
              type: DirectoryOrCreate
            name: usr-share-ca-certificates
        status: {}


[dryrun] Would write file "/etc/kubernetes/manifests/kube-controller-manager.yaml" with content:
        apiVersion: v1
        kind: Pod
        metadata:
          creationTimestamp: null
          labels:
            component: kube-controller-manager
            tier: control-plane
          name: kube-controller-manager
          namespace: kube-system
        spec:
          containers:
          - command:
            - kube-controller-manager
            - --allocate-node-cidrs=true
            - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
            - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
            - --bind-address=127.0.0.1
            - --client-ca-file=/etc/kubernetes/pki/ca.crt
            - --cluster-cidr=10.245.0.0/16
            - --cluster-name=kubernetes
            - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
            - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
            - --controllers=*,bootstrapsigner,tokencleaner
            - --kubeconfig=/etc/kubernetes/controller-manager.conf
            - --leader-elect=true
            - --node-cidr-mask-size=24
            - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
            - --root-ca-file=/etc/kubernetes/pki/ca.crt
            - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
            - --service-cluster-ip-range=10.97.0.0/12
            - --use-service-account-credentials=true
            image: k8s.gcr.io/kube-controller-manager:v1.18.2
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 8
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10257
                scheme: HTTPS
              initialDelaySeconds: 15
              timeoutSeconds: 15
            name: kube-controller-manager
            resources:
              requests:
                cpu: 200m
            volumeMounts:
            - mountPath: /etc/ssl/certs
              name: ca-certs
              readOnly: true
            - mountPath: /etc/ca-certificates
              name: etc-ca-certificates
              readOnly: true
            - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
              name: flexvolume-dir
            - mountPath: /etc/kubernetes/pki
              name: k8s-certs
              readOnly: true
            - mountPath: /etc/kubernetes/controller-manager.conf
              name: kubeconfig
              readOnly: true
            - mountPath: /usr/local/share/ca-certificates
              name: usr-local-share-ca-certificates
              readOnly: true
            - mountPath: /usr/share/ca-certificates
              name: usr-share-ca-certificates
              readOnly: true
          hostNetwork: true
          priorityClassName: system-cluster-critical
          volumes:
          - hostPath:
              path: /etc/ssl/certs
              type: DirectoryOrCreate
            name: ca-certs
          - hostPath:
              path: /etc/ca-certificates
              type: DirectoryOrCreate
            name: etc-ca-certificates
          - hostPath:
              path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
              type: DirectoryOrCreate
            name: flexvolume-dir
          - hostPath:
              path: /etc/kubernetes/pki
              type: DirectoryOrCreate
            name: k8s-certs
          - hostPath:
              path: /etc/kubernetes/controller-manager.conf
              type: FileOrCreate
            name: kubeconfig
          - hostPath:
              path: /usr/local/share/ca-certificates
              type: DirectoryOrCreate
            name: usr-local-share-ca-certificates
          - hostPath:
              path: /usr/share/ca-certificates
              type: DirectoryOrCreate
            name: usr-share-ca-certificates
        status: {}


[dryrun] Would write file "/etc/kubernetes/manifests/kube-scheduler.yaml" with content:
        apiVersion: v1
        kind: Pod
        metadata:
          creationTimestamp: null
          labels:
            component: kube-scheduler
            tier: control-plane
          name: kube-scheduler
          namespace: kube-system
        spec:
          containers:
          - command:
            - kube-scheduler
            - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
            - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
            - --bind-address=127.0.0.1
            - --kubeconfig=/etc/kubernetes/scheduler.conf
            - --leader-elect=true
            image: k8s.gcr.io/kube-scheduler:v1.18.2
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 8
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10259
                scheme: HTTPS
              initialDelaySeconds: 15
              timeoutSeconds: 15
            name: kube-scheduler
            resources:
              requests:
                cpu: 100m
            volumeMounts:
            - mountPath: /etc/kubernetes/scheduler.conf
              name: kubeconfig
              readOnly: true
          hostNetwork: true
          priorityClassName: system-cluster-critical
          volumes:
          - hostPath:
              path: /etc/kubernetes/scheduler.conf
              type: FileOrCreate
            name: kubeconfig
        status: {}


[dryrun] Would write file "/var/lib/kubelet/config.yaml" with content:
        apiVersion: kubelet.config.k8s.io/v1beta1
        authentication:
          anonymous:
            enabled: false
          webhook:
            cacheTTL: 0s
            enabled: true
          x509:
            clientCAFile: /etc/kubernetes/pki/ca.crt
        authorization:
          mode: Webhook
          webhook:
            cacheAuthorizedTTL: 0s
            cacheUnauthorizedTTL: 0s
        clusterDNS:
        - 10.97.0.10
        clusterDomain: cluster.local
        cpuManagerReconcilePeriod: 0s
        evictionPressureTransitionPeriod: 0s
        fileCheckFrequency: 0s
        healthzBindAddress: 127.0.0.1
        healthzPort: 10248
        httpCheckFrequency: 0s
        imageMinimumGCAge: 0s
        kind: KubeletConfiguration
        nodeStatusReportFrequency: 0s
        nodeStatusUpdateFrequency: 0s
        rotateCertificates: true
        runtimeRequestTimeout: 0s
        staticPodPath: /etc/kubernetes/manifests
        streamingConnectionIdleTimeout: 0s
        syncFrequency: 0s
        volumeStatsAggPeriod: 0s

[dryrun] Would write file "/var/lib/kubelet/kubeadm-flags.env" with content:
        KUBELET_KUBEADM_ARGS="--cgroup-driver=systemd --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2 --resolv-conf=/run/systemd/resolve/resolv.conf"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/tmp/kubeadm-init-dryrun674923880". This can take up to 4m0s
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace



[dryrun] Would perform action CREATE on resource "configmaps" in API group "core/v1"
[dryrun] Attached object:
        apiVersion: v1
        data:
          ClusterConfiguration: |
            apiServer:
              extraArgs:
                authorization-mode: Node,RBAC
              timeoutForControlPlane: 4m0s
            apiVersion: kubeadm.k8s.io/v1beta2
            certificatesDir: /etc/kubernetes/pki
            clusterName: kubernetes
            controllerManager: {}
            dns:
              type: CoreDNS
            etcd:
              local:
                dataDir: /var/lib/etcd
            imageRepository: k8s.gcr.io
            kind: ClusterConfiguration
            kubernetesVersion: v1.18.2
            networking:
              dnsDomain: cluster.local
              podSubnet: 10.245.0.0/16
              serviceSubnet: 10.97.0.0/12
            scheduler: {}
          ClusterStatus: |
            apiEndpoints:
              k8s-master:
                advertiseAddress: 10.244.0.38
                bindPort: 6443
            apiVersion: kubeadm.k8s.io/v1beta2
            kind: ClusterStatus
        kind: ConfigMap
        metadata:
          creationTimestamp: null
          name: kubeadm-config
          namespace: kube-system


[dryrun] Would perform action CREATE on resource "roles" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: Role
        metadata:
          creationTimestamp: null
          name: kubeadm:nodes-kubeadm-config
          namespace: kube-system
        rules:
        - apiGroups:
          - ""
          resourceNames:
          - kubeadm-config
          resources:
          - configmaps
          verbs:
          - get


[dryrun] Would perform action CREATE on resource "rolebindings" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
          creationTimestamp: null
          name: kubeadm:nodes-kubeadm-config
          namespace: kube-system
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: kubeadm:nodes-kubeadm-config
        subjects:
        - kind: Group
          name: system:bootstrappers:kubeadm:default-node-token
        - kind: Group
          name: system:nodes


[kubelet] Creating a ConfigMap "kubelet-config-1.18" in namespace kube-system with the configuration for the kubelets in the cluster
[dryrun] Would perform action CREATE on resource "configmaps" in API group "core/v1"
[dryrun] Attached object:
        apiVersion: v1
        data:
          kubelet: |
            apiVersion: kubelet.config.k8s.io/v1beta1
            authentication:
              anonymous:
                enabled: false
              webhook:
                cacheTTL: 0s
                enabled: true
              x509:
                clientCAFile: /etc/kubernetes/pki/ca.crt
            authorization:
              mode: Webhook
              webhook:
                cacheAuthorizedTTL: 0s
                cacheUnauthorizedTTL: 0s
            clusterDNS:
            - 10.97.0.10
            clusterDomain: cluster.local
            cpuManagerReconcilePeriod: 0s
            evictionPressureTransitionPeriod: 0s
            fileCheckFrequency: 0s
            healthzBindAddress: 127.0.0.1
            healthzPort: 10248
            httpCheckFrequency: 0s
            imageMinimumGCAge: 0s
            kind: KubeletConfiguration
            nodeStatusReportFrequency: 0s
            nodeStatusUpdateFrequency: 0s
            rotateCertificates: true
            runtimeRequestTimeout: 0s
            staticPodPath: /etc/kubernetes/manifests
            streamingConnectionIdleTimeout: 0s
            syncFrequency: 0s
            volumeStatsAggPeriod: 0s
        kind: ConfigMap
        metadata:
          creationTimestamp: null
          name: kubelet-config-1.18
          namespace: kube-system


[dryrun] Would perform action CREATE on resource "roles" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: Role
        metadata:
          creationTimestamp: null
          name: kubeadm:kubelet-config-1.18
          namespace: kube-system
        rules:
        - apiGroups:
          - ""
          resourceNames:
          - kubelet-config-1.18
          resources:
          - configmaps
          verbs:
          - get


[dryrun] Would perform action CREATE on resource "rolebindings" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
          creationTimestamp: null
          name: kubeadm:kubelet-config-1.18
          namespace: kube-system
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: kubeadm:kubelet-config-1.18
        subjects:
        - kind: Group
          name: system:nodes
        - kind: Group
          name: system:bootstrappers:kubeadm:default-node-token


[dryrun] Would perform action GET on resource "nodes" in API group "core/v1"
[dryrun] Resource name: "k8s-master"
[dryrun] Would perform action PATCH on resource "nodes" in API group "core/v1"
[dryrun] Resource name: "k8s-master"
[dryrun] Attached patch:
        {"metadata":{"annotations":{"kubeadm.alpha.kubernetes.io/cri-socket":"/var/run/dockershim.sock"}}}


[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]


[dryrun] Would perform action GET on resource "nodes" in API group "core/v1"
[dryrun] Resource name: "k8s-master"
[dryrun] Would perform action PATCH on resource "nodes" in API group "core/v1"
[dryrun] Resource name: "k8s-master"
[dryrun] Attached patch:
        {"metadata":{"labels":{"node-role.kubernetes.io/master":""}},"spec":{"taints":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}]}}


[bootstrap-token] Using token: i7vlcl.seczjfu2x391l64f
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles


[dryrun] Would perform action GET on resource "secrets" in API group "core/v1"
[dryrun] Resource name: "bootstrap-token-i7vlcl"


[dryrun] Would perform action CREATE on resource "secrets" in API group "core/v1"
[dryrun] Attached object:
        apiVersion: v1
        data:
          auth-extra-groups: c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=
          expiration: MjAyMS0wMy0wNlQyMToxMTo1M1o=
          token-id: aTd2bGNs
          token-secret: c2VjempmdTJ4MzkxbDY0Zg==
          usage-bootstrap-authentication: dHJ1ZQ==
          usage-bootstrap-signing: dHJ1ZQ==
        kind: Secret
        metadata:
          creationTimestamp: null
          name: bootstrap-token-i7vlcl
          namespace: kube-system
        type: bootstrap.kubernetes.io/token
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes


[dryrun] Would perform action CREATE on resource "clusterroles" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRole
        metadata:
          creationTimestamp: null
          name: kubeadm:get-nodes
          namespace: kube-system
        rules:
        - apiGroups:
          - ""
          resources:
          - nodes
          verbs:
          - get


[dryrun] Would perform action CREATE on resource "clusterrolebindings" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          creationTimestamp: null
          name: kubeadm:get-nodes
          namespace: kube-system
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: kubeadm:get-nodes
        subjects:
        - kind: Group
          name: system:bootstrappers:kubeadm:default-node-token


[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[dryrun] Would perform action CREATE on resource "clusterrolebindings" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          creationTimestamp: null
          name: kubeadm:kubelet-bootstrap
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: system:node-bootstrapper
        subjects:
        - kind: Group
          name: system:bootstrappers:kubeadm:default-node-token
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token


[dryrun] Would perform action CREATE on resource "clusterrolebindings" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          creationTimestamp: null
          name: kubeadm:node-autoapprove-bootstrap
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
        subjects:
        - kind: Group
          name: system:bootstrappers:kubeadm:default-node-token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster


[dryrun] Would perform action CREATE on resource "clusterrolebindings" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          creationTimestamp: null
          name: kubeadm:node-autoapprove-certificate-rotation
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
        subjects:
        - kind: Group
          name: system:nodes
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace


[dryrun] Would perform action CREATE on resource "configmaps" in API group "core/v1"
[dryrun] Attached object:
        apiVersion: v1
        data:
          kubeconfig: |
            apiVersion: v1
            clusters:
            - cluster:
                certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeE1ETXdOVEl4TVRFME9Gb1hEVE14TURNd016SXhNVEUwT0Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRV
BBRENDQVFvQ2dnRUJBTzVxCk55aTA0N0R3K2c5bkRqeUlLWGp6N1lPc3Z4Q29nOGxxbnJ1S0xlZVQxbkxSRzFTdGhndGV4N0hIVFh1VWY2Mk8KUm1RSDhSbmRqNm4zRGNITmVzOE1FeU9YeEczemRuMXQ3WHEzYXdVbkRkVnF5SzdSaDFCU0V1Si9yS1hDcWZRKwpyMmF4ODN4RnpFQkJZcFJVQ3E1M0pONDBsU2w5NUtJYUJ1VDdyNlRQOTNMMEVqRXNVWkE3Qis1aE5tYnAvVVpxCnBuenI5UFl1VE5YZTR2TkYrSHp6SjNrZ3do
bExlSGVteGdhVzZ0My93UVRmY0Z3RWYxVkNOOUt2U0RKcHBicGoKbmNDc0FKRjRFSGpUZkhudm9vUnc3UFlwWUtOOUVDakNqN0JxS3hkWCt4K3RTY1lvSUwrV1lBMU5ZL2g4RCtRdwpFYUl2Kzh6Y0gyWndlNTdmbXhjQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFBNlQvek9VaEg4VWMzdkU3V0UxSEpGRGhHdlUKWkVzUG
RXdE1OSU05N1pTNFpSU2F6WGN6RlJDWU5DRitYWWxuMS9JQ0ZQdGVzeHB4cm1EbjNxQ3IwQU5jV1VFbwpXa2t2LzN1TG1FUGdld1dya2d4alZvbTV0U0lVQ3J6eXVURlI3ZnMyZGVjVy9aQlNrSzBPdS9Fby93TlEyZERoCkI2d2tRMkZuOTNZVFN0SFNocURaK0lraXBMcUU3NUppbVloVmN2eU9IU0pPYU5ZS0RPRThIaktPSWluVFpZZFAKRkRjTkJVd2UyTDRtaGk1aHVGVDU4ZFF2NG1KMVU3WGcxZ251ZlBRZDVRODZUemcz
RjlCdWV1MndXUEpVTFVPZAoxaTI2SzQ4N1lzMUtmNHVFMzBjZ1hqNTlYblNya1d1Ump5T3k0RTM3SUhDQmY4Tm5DV1RvOWY1Rk1yTT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
                server: https://10.244.0.38:6443
              name: ""
            contexts: null
            current-context: ""
            kind: Config
            preferences: {}
            users: null
        kind: ConfigMap
        metadata:
          creationTimestamp: null
          name: cluster-info
          namespace: kube-public


[dryrun] Would perform action CREATE on resource "roles" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: Role
        metadata:
          creationTimestamp: null
          name: kubeadm:bootstrap-signer-clusterinfo
          namespace: kube-public
        rules:
        - apiGroups:
          - ""
          resourceNames:
          - cluster-info
          resources:
          - configmaps
          verbs:
          - get


[dryrun] Would perform action CREATE on resource "rolebindings" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
          creationTimestamp: null
          name: kubeadm:bootstrap-signer-clusterinfo
          namespace: kube-public
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: kubeadm:bootstrap-signer-clusterinfo
        subjects:
        - kind: User
          name: system:anonymous


[dryrun] Would perform action LIST on resource "deployments" in API group "apps/v1"
[dryrun] Would perform action GET on resource "configmaps" in API group "core/v1"
[dryrun] Resource name: "kube-dns"
[dryrun] Would perform action GET on resource "configmaps" in API group "core/v1"
[dryrun] Resource name: "coredns"


[dryrun] Would perform action CREATE on resource "configmaps" in API group "core/v1"
[dryrun] Attached object:
        apiVersion: v1
        data:
          Corefile: |
            .:53 {
                errors
                health {
                   lameduck 5s
                }
                ready
                kubernetes cluster.local in-addr.arpa ip6.arpa {
                   pods insecure
                   fallthrough in-addr.arpa ip6.arpa
                   ttl 30
                }
                prometheus :9153
                forward . /etc/resolv.conf
                cache 30
                loop
                reload
                loadbalance
            }
        kind: ConfigMap
        metadata:
          creationTimestamp: null
          name: coredns
          namespace: kube-system


[dryrun] Would perform action CREATE on resource "clusterroles" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRole
        metadata:
          creationTimestamp: null
          name: system:coredns
        rules:
        - apiGroups:
          - ""
          resources:
          - endpoints
          - services
          - pods
          - namespaces
          verbs:
          - list
          - watch
        - apiGroups:
          - ""
          resources:
          - nodes
          verbs:
          - get


[dryrun] Would perform action CREATE on resource "clusterrolebindings" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          creationTimestamp: null
          name: system:coredns
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: system:coredns
        subjects:
        - kind: ServiceAccount
          name: coredns
          namespace: kube-system


[dryrun] Would perform action CREATE on resource "serviceaccounts" in API group "core/v1"
[dryrun] Attached object:
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          creationTimestamp: null
          name: coredns
          namespace: kube-system


[dryrun] Would perform action CREATE on resource "deployments" in API group "apps/v1"
[dryrun] Attached object:
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: kube-dns
          name: coredns
          namespace: kube-system
        spec:
          replicas: 2
          selector:
            matchLabels:
              k8s-app: kube-dns
          strategy:
            rollingUpdate:
              maxUnavailable: 1
            type: RollingUpdate
          template:
            metadata:
              creationTimestamp: null
              labels:
                k8s-app: kube-dns
            spec:
              containers:
              - args:
                - -conf
                - /etc/coredns/Corefile
                image: k8s.gcr.io/coredns:1.6.7
                imagePullPolicy: IfNotPresent
                livenessProbe:
                  failureThreshold: 5
                  httpGet:
                    path: /health
                    port: 8080
                    scheme: HTTP
                  initialDelaySeconds: 60
                  successThreshold: 1
                  timeoutSeconds: 5
                name: coredns
                ports:
                - containerPort: 53
                  name: dns
                  protocol: UDP
                - containerPort: 53
                  name: dns-tcp
                  protocol: TCP
                - containerPort: 9153
                  name: metrics
                  protocol: TCP
                readinessProbe:
                  httpGet:
                    path: /ready
                    port: 8181
                    scheme: HTTP
                resources:
                  limits:
                    memory: 170Mi
                  requests:
                    cpu: 100m
                    memory: 70Mi
                securityContext:
                  allowPrivilegeEscalation: false
                  capabilities:
                    add:
                    - NET_BIND_SERVICE
                    drop:
                    - all
                  readOnlyRootFilesystem: true
                volumeMounts:
                - mountPath: /etc/coredns
                  name: config-volume
                  readOnly: true
              dnsPolicy: Default
              nodeSelector:
                kubernetes.io/os: linux
              priorityClassName: system-cluster-critical
              serviceAccountName: coredns
              tolerations:
              - key: CriticalAddonsOnly
                operator: Exists
              - effect: NoSchedule
                key: node-role.kubernetes.io/master
              volumes:
              - configMap:
                  items:
                  - key: Corefile
                    path: Corefile
                  name: coredns
                name: config-volume
        status: {}


[dryrun] Would perform action CREATE on resource "services" in API group "core/v1"
[dryrun] Attached object:
        apiVersion: v1
        kind: Service
        metadata:
          annotations:
            prometheus.io/port: "9153"
            prometheus.io/scrape: "true"
          creationTimestamp: null
          labels:
            k8s-app: kube-dns
            kubernetes.io/cluster-service: "true"
            kubernetes.io/name: KubeDNS
          name: kube-dns
          namespace: kube-system
          resourceVersion: "0"
        spec:
          clusterIP: 10.96.0.10    <<< ERROR: should be 10.97.0.10
          ports:
          - name: dns
            port: 53
            protocol: UDP
            targetPort: 53
          - name: dns-tcp
            port: 53
            protocol: TCP
            targetPort: 53
          - name: metrics
            port: 9153
            protocol: TCP
            targetPort: 9153
          selector:
            k8s-app: kube-dns
        status:
          loadBalancer: {}


[addons] Applied essential addon: CoreDNS
[dryrun] Would perform action CREATE on resource "serviceaccounts" in API group "core/v1"
[dryrun] Attached object:
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          creationTimestamp: null
          name: kube-proxy
          namespace: kube-system


[dryrun] Would perform action CREATE on resource "configmaps" in API group "core/v1"
[dryrun] Attached object:
        apiVersion: v1
        data:
          config.conf: |-
            apiVersion: kubeproxy.config.k8s.io/v1alpha1
            bindAddress: 0.0.0.0
            clientConnection:
              acceptContentTypes: ""
              burst: 0
              contentType: ""
              kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
              qps: 0
            clusterCIDR: 10.245.0.0/16
            configSyncPeriod: 0s
            conntrack:
              maxPerCore: null
              min: null
              tcpCloseWaitTimeout: null
              tcpEstablishedTimeout: null
            detectLocalMode: ""
            enableProfiling: false
            healthzBindAddress: ""
            hostnameOverride: ""
            iptables:
              masqueradeAll: false
              masqueradeBit: null
              minSyncPeriod: 0s
              syncPeriod: 0s
            ipvs:
              excludeCIDRs: null
              minSyncPeriod: 0s
              scheduler: ""
              strictARP: false
              syncPeriod: 0s
              tcpFinTimeout: 0s
              tcpTimeout: 0s
              udpTimeout: 0s
            kind: KubeProxyConfiguration
            metricsBindAddress: ""
            mode: ""
            nodePortAddresses: null
            oomScoreAdj: null
            portRange: ""
            showHiddenMetricsForVersion: ""
            udpIdleTimeout: 0s
            winkernel:
              enableDSR: false
              networkName: ""
              sourceVip: ""
          kubeconfig.conf: |-
            apiVersion: v1
            kind: Config
            clusters:
            - cluster:
                certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                server: https://10.244.0.38:6443
              name: default
            contexts:
            - context:
                cluster: default
                namespace: default
                user: default
              name: default
            current-context: default
            users:
            - name: default
              user:
                tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
        kind: ConfigMap
        metadata:
          creationTimestamp: null
          labels:
            app: kube-proxy
          name: kube-proxy
          namespace: kube-system


[dryrun] Would perform action CREATE on resource "daemonsets" in API group "apps/v1"
[dryrun] Attached object:
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: kube-proxy
          name: kube-proxy
          namespace: kube-system
        spec:
          selector:
            matchLabels:
              k8s-app: kube-proxy
          template:
            metadata:
              creationTimestamp: null
              labels:
                k8s-app: kube-proxy
            spec:
              containers:
              - command:
                - /usr/local/bin/kube-proxy
                - --config=/var/lib/kube-proxy/config.conf
                - --hostname-override=$(NODE_NAME)
                env:
                - name: NODE_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.nodeName
                image: k8s.gcr.io/kube-proxy:v1.18.2
                imagePullPolicy: IfNotPresent
                name: kube-proxy
                resources: {}
                securityContext:
                  privileged: true
                volumeMounts:
                - mountPath: /var/lib/kube-proxy
                  name: kube-proxy
                - mountPath: /run/xtables.lock
                  name: xtables-lock
                - mountPath: /lib/modules
                  name: lib-modules
                  readOnly: true
              hostNetwork: true
              nodeSelector:
                kubernetes.io/os: linux
              priorityClassName: system-node-critical
              serviceAccountName: kube-proxy
              tolerations:
              - key: CriticalAddonsOnly
                operator: Exists
              - operator: Exists
              volumes:
              - configMap:
                  name: kube-proxy
                name: kube-proxy
              - hostPath:
                  path: /run/xtables.lock
                  type: FileOrCreate
                name: xtables-lock
              - hostPath:
                  path: /lib/modules
                name: lib-modules
          updateStrategy:
            type: RollingUpdate
        status:
          currentNumberScheduled: 0
          desiredNumberScheduled: 0
          numberMisscheduled: 0
          numberReady: 0


[dryrun] Would perform action CREATE on resource "clusterrolebindings" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          creationTimestamp: null
          name: kubeadm:node-proxier
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: system:node-proxier
        subjects:
        - kind: ServiceAccount
          name: kube-proxy
          namespace: kube-system


[dryrun] Would perform action CREATE on resource "roles" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: Role
        metadata:
          creationTimestamp: null
          name: kube-proxy
          namespace: kube-system
        rules:
        - apiGroups:
          - ""
          resourceNames:
          - kube-proxy
          resources:
          - configmaps
          verbs:
          - get


[dryrun] Would perform action CREATE on resource "rolebindings" in API group "rbac.authorization.k8s.io/v1"
[dryrun] Attached object:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
          creationTimestamp: null
          name: kube-proxy
          namespace: kube-system
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: kube-proxy
        subjects:
        - kind: Group
          name: system:bootstrappers:kubeadm:default-node-token


[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/tmp/kubeadm-init-dryrun674923880/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.244.0.38:6443 --token i7vlcl.seczjfu2x391l64f \
    --discovery-token-ca-cert-hash sha256:5f8668b8bc919c7102053b5afe912d7008401620aca0c716d7c83553d0ffebd0
```
